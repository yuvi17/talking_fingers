{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tree import ParentedTree, Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "os.environ['STANFORD_PARSER'] = '/usr/local/Cellar/stanford-parser/3.8.0/libexec'\n",
    "os.environ['STANFORD_MODELS'] = '/usr/local/Cellar/stanford-parser/3.8.0/libexec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StanfordParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = list(parser.raw_parse(\"What is your name?\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = ParentedTree.convert(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subject(t):\n",
    "  for s in t.subtrees(lambda t: t.label() == 'NP'):\n",
    "    for n in s.subtrees(lambda n: n.label().startswith('NN')):\n",
    "      if(n != None):\n",
    "          return (n[0], find_attrs(n))\n",
    "    for n in s.subtrees(lambda n: n.label().startswith('PRP')):\n",
    "      if(n != None):\n",
    "          return (n[0], find_attrs(n))\n",
    "    \n",
    "def findQuestionMatter(t):\n",
    "    ans = []\n",
    "    for s in t.subtrees(lambda t: t.label().startswith(\"WH\")):\n",
    "      for n in s.subtrees(lambda n: n.label().startswith('WHADJP')):\n",
    "        for n in s.subtrees((lambda n: n.label() == ('JJ'))):\n",
    "          ans.append(n[0])\n",
    "      for n in s.subtrees((lambda n: n.label().startswith('NN'))):\n",
    "        if(n not in ans):\n",
    "            ans.append(n)\n",
    "      for n in s.subtrees(lambda n: n.label().startswith('WH')):\n",
    "        for n in s.subtrees((lambda n: n.label() == ('WRB'))):\n",
    "          ans.append(n)\n",
    "        for n in s.subtrees((lambda n: n.label().startswith('WP'))):\n",
    "          ans.append(n)\n",
    "        for n in s.subtrees((lambda n: n.label().startswith('WD'))):\n",
    "          ans.append(n)\n",
    "    return ans\n",
    " \n",
    "# Depth First Search the tree and take the last verb in VP subtree.\n",
    "def find_predicate(t):\n",
    "  v = None\n",
    " \n",
    "  for s in t.subtrees(lambda t: t.label() == 'VP'):\n",
    "    for n in s.subtrees(lambda n: n.label().startswith('VB')):\n",
    "      v = n\n",
    "  if(v != None):\n",
    "      return (v[0], find_attrs(v))\n",
    " \n",
    "# Breadth First Search the siblings of VP subtree\n",
    "# and take the first noun or adjective\n",
    "def find_object(t):\n",
    "  for s in t.subtrees(lambda t: t.label() == 'VP'):\n",
    "    for n in s.subtrees(lambda n: n.label() in ['NP', 'PP', 'ADJP']):\n",
    "      if n.label() in ['NP', 'PP']:\n",
    "        for c in n.subtrees(lambda c: c.label().startswith('NN')):\n",
    "          return (c[0], find_attrs(c))\n",
    "      else:\n",
    "        for c in n.subtrees(lambda c: c.label().startswith('JJ')):\n",
    "          return (c[0], find_attrs(c))\n",
    " \n",
    "def find_attrs(node):\n",
    "  attrs = []\n",
    "  p = node.parent()\n",
    " \n",
    "  # Search siblings of adjective for adverbs\n",
    "  if node.label().startswith('JJ'):\n",
    "    for s in p:\n",
    "      if s.label() == 'RB':\n",
    "        attrs.append(s[0])\n",
    " \n",
    "  elif node.label().startswith('NN'):\n",
    "    for s in p:\n",
    "      if s.label() in ['DT','PRP$','POS','JJ','CD','ADJP','QP','NP']:\n",
    "        attrs.append(s[0])\n",
    "\n",
    "  elif node.label().startswith('NP'):\n",
    "    for s in p:\n",
    "      if s.label() in ['PRP$','NN']:\n",
    "        attrs.append(s[0])\n",
    " \n",
    "  # Search siblings of verbs for adverb phrase\n",
    "  elif node.label().startswith('VB'):\n",
    "    for s in p:\n",
    "      if s.label() == 'ADVP':\n",
    "        attrs.append(' '.join(s.flatten()))\n",
    " \n",
    "  # Search uncles\n",
    "  # if the node is noun or adjective search for prepositional phrase\n",
    "  if node.label().startswith('JJ') or node.label().startswith('NN'):\n",
    "      for s in p.parent():\n",
    "        if s != p and s.label() == 'PP':\n",
    "          attrs.append(' '.join(s.flatten()))\n",
    " \n",
    "  elif node.label().startswith('VB'):\n",
    "    for s in p.parent():\n",
    "      if s != p and s.label().startswith('VB'):\n",
    "        attrs.append(' '.join(s.flatten()))\n",
    " \n",
    "  return attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(find_attrs(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseSentence(sentence):\n",
    "    import re\n",
    "    stopwords = [\"are\",\"is\",\"am\",\"was\",\"were\",\"a\",\"an\",\"the\",\"do\",\"did\",\"Did\"]\n",
    "    \n",
    "    replace = {\n",
    "        'many' : 'number',\n",
    "        'I' : 'me',\n",
    "        'my' : 'me'\n",
    "    }\n",
    "    \n",
    "    stopwords = set(stopwords)\n",
    "    res = list(parser.raw_parse(sentence))[0]\n",
    "    print(res)\n",
    "    res = ParentedTree.convert(res)\n",
    "    res.pretty_print()\n",
    "    subjects = find_subject(res)\n",
    "    if(subjects == None):\n",
    "        subjects = (\"\",[])\n",
    "    objects = find_object(res)\n",
    "    if(objects == None):\n",
    "        objects = (\"\",[])\n",
    "    verbs = find_predicate(res)\n",
    "    if(verbs == None):\n",
    "        verbs = (\"\",[])\n",
    "    attributes = find_attrs(res)\n",
    "    if(attributes == None):\n",
    "        attributes = (\"\",[])\n",
    "    ques = findQuestionMatter(res)\n",
    "    if(ques == None):\n",
    "        ques = (\"\",[])\n",
    "    print(subjects)\n",
    "    print(objects)\n",
    "    print(verbs)\n",
    "    print(attributes)\n",
    "    print(ques)\n",
    "    tags = pos_tag(word_tokenize(sentence))\n",
    "    print(pos_tag(word_tokenize(sentence)))\n",
    "    \n",
    "    sent = \"\"\n",
    "    for s in subjects[1]:\n",
    "        if(s not in stopwords):\n",
    "            sent = sent + \" \" + s + \" \"\n",
    "    if(subjects[0] not in sent):\n",
    "        sent = sent + \" \" + subjects[0] + \" \"\n",
    "    for o in objects[1]:\n",
    "        if(o not in stopwords and o not in sent):\n",
    "            sent = sent + \" \" + o + \" \"\n",
    "    if(objects[0] not in sent and objects[0] not in stopwords):\n",
    "        sent = sent + \" \" + ps.stem(objects[0]) + \" \"\n",
    "    \n",
    "    for v in verbs[1]:\n",
    "        if(v not in stopwords):\n",
    "            sent = sent + \" \" + ps.stem(v) + \" \"\n",
    "    if(verbs[0] not in sent and ps.stem(verbs[0]) not in stopwords):\n",
    "        sent = sent + \" \" + ps.stem(verbs[0]) + \" \"\n",
    "    for q in ques:\n",
    "        if(q[0] not in stopwords and q[0] not in set(sent)):\n",
    "            sent = sent + \" \" + q[0] + \" \"\n",
    "    for t in tags:\n",
    "        if(t[1] == \"JJS\"):\n",
    "            sent = sent + \" \" + t[0] + \" \"\n",
    "    return re.sub( '\\s+', ' ', sent ).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S (NP (PRP I)) (VP (VBP am) (PP (IN from) (NP (NNP Himachal))))))\n",
      "        ROOT                  \n",
      "         |                     \n",
      "         S                    \n",
      "  _______|____                 \n",
      " |            VP              \n",
      " |    ________|____            \n",
      " |   |             PP         \n",
      " |   |         ____|_____      \n",
      " NP  |        |          NP   \n",
      " |   |        |          |     \n",
      "PRP VBP       IN        NNP   \n",
      " |   |        |          |     \n",
      " I   am      from     Himachal\n",
      "\n",
      "('I', [])\n",
      "('Himachal', [])\n",
      "('am', [])\n",
      "[]\n",
      "[]\n",
      "[('I', 'PRP'), ('am', 'VBP'), ('from', 'IN'), ('Himachal', 'NNP')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I himach'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parseSentence(\"I am from Himachal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'offic'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"office\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ran'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"ran\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"../vocab/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "file = 'vocab.csv'\n",
    "with open(file, 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    for val in files:\n",
    "        wr.writerow([val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'article'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-e0850c8b136b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/yjaiswal/Development/eng2isl/parsingEngine/en/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m######################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommonsense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumeral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'article'"
     ]
    }
   ],
   "source": [
    "import en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autocorrect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-388683e21242>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautocorrect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autocorrect'"
     ]
    }
   ],
   "source": [
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wa'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"was\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
