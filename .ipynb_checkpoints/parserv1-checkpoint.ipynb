{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tree import ParentedTree, Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "os.environ['STANFORD_PARSER'] = '/usr/local/Cellar/stanford-parser/3.8.0/libexec'\n",
    "os.environ['STANFORD_MODELS'] = '/usr/local/Cellar/stanford-parser/3.8.0/libexec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StanfordParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = list(parser.raw_parse(\"What is your name?\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = ParentedTree.convert(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subject(t):\n",
    "  for s in t.subtrees(lambda t: t.label() == 'NP'):\n",
    "    for n in s.subtrees(lambda n: n.label().startswith('NN')):\n",
    "      if(n != None):\n",
    "          return (n[0], find_attrs(n))\n",
    "    for n in s.subtrees(lambda n: n.label().startswith('PRP')):\n",
    "      if(n != None):\n",
    "          return (n[0], find_attrs(n))\n",
    "    \n",
    "def findQuestionMatter(t):\n",
    "    ans = []\n",
    "    for s in t.subtrees(lambda t: t.label().startswith(\"WH\")):\n",
    "      for n in s.subtrees(lambda n: n.label().startswith('WHADJP')):\n",
    "        for n in s.subtrees((lambda n: n.label() == ('JJ'))):\n",
    "          ans.append(n[0],find_attrs(n))\n",
    "      for n in s.subtrees((lambda n: n.label().startswith('NN'))):\n",
    "        if((n[0],find_attrs(n)) not in ans):\n",
    "            ans.append(n[0])\n",
    "      for n in s.subtrees(lambda n: n.label().startswith('WH')):\n",
    "        for n in s.subtrees((lambda n: n.label() == ('WRB'))):\n",
    "          ans.append(n[0],)\n",
    "        for n in s.subtrees((lambda n: n.label().startswith('WP'))):\n",
    "          ans.append(n[0],'')\n",
    "    return ans\n",
    " \n",
    "# Depth First Search the tree and take the last verb in VP subtree.\n",
    "def find_predicate(t):\n",
    "  v = None\n",
    " \n",
    "  for s in t.subtrees(lambda t: t.label() == 'VP'):\n",
    "    for n in s.subtrees(lambda n: n.label().startswith('VB')):\n",
    "      v = n\n",
    "  if(v != None):\n",
    "      return (v[0], find_attrs(v))\n",
    " \n",
    "# Breadth First Search the siblings of VP subtree\n",
    "# and take the first noun or adjective\n",
    "def find_object(t):\n",
    "  for s in t.subtrees(lambda t: t.label() == 'VP'):\n",
    "    for n in s.subtrees(lambda n: n.label() in ['NP', 'PP', 'ADJP']):\n",
    "      if n.label() in ['NP', 'PP']:\n",
    "        for c in n.subtrees(lambda c: c.label().startswith('NN')):\n",
    "          return (c[0], find_attrs(c))\n",
    "      else:\n",
    "        for c in n.subtrees(lambda c: c.label().startswith('JJ')):\n",
    "          return (c[0], find_attrs(c))\n",
    " \n",
    "def find_attrs(node):\n",
    "  attrs = []\n",
    "  p = node.parent()\n",
    " \n",
    "  # Search siblings of adjective for adverbs\n",
    "  if node.label().startswith('JJ'):\n",
    "    for s in p:\n",
    "      if s.label() == 'RB':\n",
    "        attrs.append(s[0])\n",
    " \n",
    "  elif node.label().startswith('NN'):\n",
    "    for s in p:\n",
    "      if s.label() in ['DT','PRP$','POS','JJ','CD','ADJP','QP','NP']:\n",
    "        attrs.append(s[0])\n",
    "\n",
    "  elif node.label().startswith('NP'):\n",
    "    for s in p:\n",
    "      if s.label() in ['PRP$','NN']:\n",
    "        attrs.append(s[0])\n",
    " \n",
    "  # Search siblings of verbs for adverb phrase\n",
    "  elif node.label().startswith('VB'):\n",
    "    for s in p:\n",
    "      if s.label() == 'ADVP':\n",
    "        attrs.append(' '.join(s.flatten()))\n",
    " \n",
    "  # Search uncles\n",
    "  # if the node is noun or adjective search for prepositional phrase\n",
    "  if node.label().startswith('JJ') or node.label().startswith('NN'):\n",
    "      for s in p.parent():\n",
    "        if s != p and s.label() == 'PP':\n",
    "          attrs.append(' '.join(s.flatten()))\n",
    " \n",
    "  elif node.label().startswith('VB'):\n",
    "    for s in p.parent():\n",
    "      if s != p and s.label().startswith('VB'):\n",
    "        attrs.append(' '.join(s.flatten()))\n",
    " \n",
    "  return attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ROOT                          \n",
      "           |                             \n",
      "           S                            \n",
      "  _________|__________________________   \n",
      " |               VP                   | \n",
      " |    ___________|_____               |  \n",
      " |   |                 VP             | \n",
      " |   |      ___________|_______       |  \n",
      " NP  |     |           NP      NP     | \n",
      " |   |     |           |       |      |  \n",
      "PRP  MD    VB          NN      NN     . \n",
      " |   |     |           |       |      |  \n",
      " I  will attend     meeting tomorrow  ? \n",
      "\n",
      "('I', [])\n",
      "('meeting', [])\n",
      "('attend', [])\n",
      "[]\n",
      "[]\n",
      "[('I', 'PRP'), ('will', 'MD'), ('attend', 'VB'), ('meeting', 'NN'), ('tomorrow', 'NN'), ('?', '.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I meet attend'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parseSentence(\"I will attend meeting tomorrow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_attrs(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseSentence(sentence):\n",
    "    import re\n",
    "    stopwords = [\"are\",\"is\",\"am\",\"was\",\"were\",\"a\",\"an\",\"the\",\"do\",\"did\"]\n",
    "    \n",
    "    replace = {\n",
    "        'many' : 'number',\n",
    "        'I' : 'me',\n",
    "        'my' : 'me'\n",
    "    }\n",
    "    \n",
    "    stopwords = set(stopwords)\n",
    "    res = list(parser.raw_parse(sentence))[0]\n",
    "    print(res)\n",
    "    res = ParentedTree.convert(res)\n",
    "    res.pretty_print()\n",
    "    subjects = find_subject(res)\n",
    "    if(subjects == None):\n",
    "        subjects = (\"\",[])\n",
    "    objects = find_object(res)\n",
    "    if(objects == None):\n",
    "        objects = (\"\",[])\n",
    "    verbs = find_predicate(res)\n",
    "    if(verbs == None):\n",
    "        verbs = (\"\",[])\n",
    "    attributes = find_attrs(res)\n",
    "    if(attributes == None):\n",
    "        attributes = (\"\",[])\n",
    "    ques = findQuestionMatter(res)\n",
    "    if(ques == None):\n",
    "        ques = (\"\",[])\n",
    "    print(subjects)\n",
    "    print(objects)\n",
    "    print(verbs)\n",
    "    print(attributes)\n",
    "    print(ques)\n",
    "    print(pos_tag(word_tokenize(sentence)))\n",
    "    \n",
    "    sent = \"\"\n",
    "    for s in subjects[1]:\n",
    "        if(s not in stopwords):\n",
    "            sent = sent + \" \" + s + \" \"\n",
    "    if(subjects[0] not in sent):\n",
    "        sent = sent + \" \" + subjects[0] + \" \"\n",
    "    for o in objects[1]:\n",
    "        if(o not in stopwords and o not in sent):\n",
    "            sent = sent + \" \" + o + \" \"\n",
    "    if(objects[0] not in sent and objects[0] not in stopwords):\n",
    "        sent = sent + \" \" + ps.stem(objects[0]) + \" \"\n",
    "    \n",
    "    for v in verbs[1]:\n",
    "        if(v not in stopwords):\n",
    "            sent = sent + \" \" + ps.stem(v) + \" \"\n",
    "    if(verbs[0] not in sent and ps.stem(verbs[0]) not in stopwords):\n",
    "        sent = sent + \" \" + ps.stem(verbs[0]) + \" \"\n",
    "    for q in ques:\n",
    "        if(q[0] not in stopwords and q[0] not in set(sent)):\n",
    "            sent = sent + \" \" + q[0] + \" \"\n",
    "    return re.sub( '\\s+', ' ', sent ).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ROOT                  \n",
      "              |                     \n",
      "              S                    \n",
      "  ____________|______               \n",
      " |                   VP            \n",
      " |      _____________|________      \n",
      " NP    |             NP       NP   \n",
      " |     |             |        |     \n",
      "PRP   VBD            NN       NN   \n",
      " |     |             |        |     \n",
      " I  attended      meeting yesterday\n",
      "\n",
      "('I', [])\n",
      "('meeting', [])\n",
      "('attended', [])\n",
      "[]\n",
      "[]\n",
      "[('I', 'PRP'), ('attended', 'VBD'), ('meeting', 'NN'), ('yesterday', 'NN')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I meet attend'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parseSentence(\"I attended meeting yesterday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ran'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"ran\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ran'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"ran\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(swn.senti_synsets('lunch.n.01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
